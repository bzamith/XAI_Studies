{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e979d0d4-52b4-4f75-9f58-fa78cee65bec",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    \n",
    "# <strong>Atividade 3: Visão Geral de Explicabilidade</strong>\n",
    "\n",
    "#### Tópicos Avançados em IC 2 - 2024.2\n",
    "\n",
    "#### Universidade Federal de Pernambuco (UFPE)\n",
    "\n",
    "---\n",
    "\n",
    "**Aluna:** Bruna Zamith Santos  \n",
    "\n",
    "**Professor:** Dr. Ricardo Prudêncio\n",
    "\n",
    "**Data:** 02/09/2024\n",
    "\n",
    "---\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6137d-b791-4fea-b23e-fe9e7c97d804",
   "metadata": {},
   "source": [
    "> P.S: As respostas a seguir são baseadas no livro [\"Interpretable Machine Learning\", Christoph Molnar](https://christophm.github.io/interpretable-ml-book/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44601915-ee61-4601-960d-b65c929cfc68",
   "metadata": {},
   "source": [
    "##  Em que situações, prover explicações de modelos não é importante?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7e3b7-4934-4fa0-baff-5ccde6f17a3f",
   "metadata": {},
   "source": [
    "Em alguns casos, não é relevante prover explicações de modelo, pois saber apenas a previsão do modelo é suficiente. Como por exemplo:\n",
    "\n",
    "1. **Nenhum impacto significativo**: Alguns modelos podem não requerer explicações pois são usados em um ambiente de baixo risco - ou seja, erros não terão consequência graves (e.g., um sistema de recomendação de vídeos/músicas);\n",
    "2. **O problema/método já é amplamente estudado e avaliado**: Algumas aplicações foram tão bem estudadas que os problemas do modelo foram resolvidos com a experiência prática. Por exemplo, o problema de reconhecimento óptico de caracteres.\n",
    "3. **Risco de manipulação do sistema**: Em alguns casos, a interpretabilidade pode gerar riscos de usuários mal intencionados manipularem indevidamente o sistema. Nesses casos, a interpretabilidade não é desejável. Modelos de crédito são um exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b0a70-a569-4a6f-bf75-6209dab8c331",
   "metadata": {},
   "source": [
    "## Qual é a diferença entre explicações intrínsicas e explicações post hoc?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbaba45-cf27-4b2f-853f-e1a42b01ec28",
   "metadata": {},
   "source": [
    "Métodos que geram explicações intrínsecas referem-se a modelos que são considerados interpretáveis devido à sua estrutura, como por exemplo árvores de decisão e modelos lineares (modelos \"white-box\"). \n",
    "\n",
    "Já os métodos que geram explicações post-hoc atingem à interepretabilidade depois que o modelo já foi treinado (modelos \"black-box\"). Exemplos são o SHAP, permutation feature importance, e outros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83657a5-9298-4b37-9e79-ebfe6d62ea2e",
   "metadata": {},
   "source": [
    "## O que é interpretabilidade local?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742dca0-a686-4637-9efa-542bd2d888f3",
   "metadata": {},
   "source": [
    "Interpretabilidade local refere-se à capacidade de entender quais fatores levaram um modelo de Machine Learning (ML) a prever um resultado específico para uma instância ou grupo de instâncias. Em outras palavras, a partir de uma instância ou grupo de instâncias, são geradas explicações que tornam os resultados compreensíveis para seres humanos. Isso contrasta com a interpretabilidade global, que busca explicações para o modelo como um todo, sem focar em instâncias específicas.\n",
    "\n",
    "Um exemplo de interpretabilidade local ocorre quando um cliente de banco deseja saber por que seu pedido de crédito foi negado por um modelo de ML, ou quando um paciente quer entender por que um modelo de ML o diagnosticou com uma determinada doença. Um exemplo de método de interpretabilidade local é o LIME (Local-Intepretable Model agnostic Explanations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5a831-2e65-4fa4-b591-04ce633a7aba",
   "metadata": {},
   "source": [
    "## O que são métodos agnósticos para explicabilidade?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d14254-3496-4e3f-980f-8a0dd7023904",
   "metadata": {},
   "source": [
    "São métodos que independem do algoritmo usado para treinar o modelo. Ou seja, podem gerar explicações para qualquer modelo de ML - isso porque eles tratam modelos como \"black-boxes\" e, assim, não assumem quaisquer parâmetros ou estruturas intrínsecas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d09abc-eb5d-47d1-bd29-0b3c41ebb06c",
   "metadata": {},
   "source": [
    "## Considerando as propriedades de explicações individuais, qual a diferença entre acurácia e fidelidade de explicações?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535a531-51ac-4463-836d-d063ddb65170",
   "metadata": {},
   "source": [
    "- **Acurácia**: Refere-se à capacidade de uma explicação prever dados que ainda não foram vistos. Em outras palavras, mede o quão bem a explicação pode ser usada para prever novos dados, especialmente se ela estiver sendo usada no lugar do modelo de ML. \n",
    "- **Fidelidade**: Avalia quão bem a explicação se aproxima das previsões feitas pelo modelo de ML. A fidelidade é crucial, pois uma explicação com baixa fidelidade não serve para explicar adequadamente o modelo.\n",
    "\n",
    "Embora acurácia e fidelidade estejam relacionadas, a fidelidade é mais focada em quão bem a explicação reflete as decisões do modelo, enquanto a acurácia se concentra na previsão de dados novos. Além disso, algumas explicações podem ter fidelidade apenas local, ou seja, aproximam bem as previsões do modelo apenas para um subconjunto dos dados ou para uma instância específica.\n",
    "\n",
    "Portanto, a principal diferença entre acurácia e fidelidade é o foco de cada uma. A acurácia é sobre a capacidade preditiva da explicação, enquanto a fidelidade é sobre o quão bem a explicação replica as previsões do modelo original. Uma explicação com alta fidelidade garante que estamos entendendo corretamente o que o modelo faz, mesmo que essa explicação não tenha necessariamente uma alta acurácia preditiva em dados novos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
